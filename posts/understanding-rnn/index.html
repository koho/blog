<!doctype html><html lang=zh-cn id=page><head><meta charset=utf-8><title>理解 RNN - xhat's Blog</title><meta name=viewport content="width=device-width,initial-scale=1"><link href=/css/font.css type=text/css rel=stylesheet><link href=/css/main.css type=text/css rel=stylesheet><link href=/css/github-markdown-light.css type=text/css rel=stylesheet><link rel=icon href=/favicon.svg></head><body><header><a href=/ id=logo><svg xmlns="http://www.w3.org/2000/svg" width="38.82" height="52.68" viewBox="0 -765 572 776" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="MJX-9-TEX-I-78" d="M52 289q7 42 54 97t116 56q35 0 64-18t43-45q42 63 101 63 37 0 64-22t28-59q0-29-14-47t-27-22-23-4q-19 0-31 11t-12 29q0 46 50 63-11 13-40 13-13 0-19-2-38-16-56-66-60-221-60-258 0-28 16-40t35-12q37 0 73 33t49 81q3 10 6 11t16 2h4q15 0 15-8 0-1-2-11-16-57-62-101T333-11q-70 0-106 63-41-62-94-62h-6Q78-10 57 16T35 71q0 32 19 52t45 20q43 0 43-42 0-20-12-35T107 46 94 41l-3-1q0-1 6-4t16-7 19-3q36 0 62 45 9 16 23 68t28 108 16 66q5 27 5 39 0 28-15 40t-34 12q-40 0-75-32T93 290q-2-9-5-10t-16-2H58q-6 6-6 11z"/><path id="MJX-9-TEX-N-5E" d="M112 560 249 694l8-8Q387 562 387 560l-26-29q-2 1-58 50l-53 46-55-47q-13-11-26-23t-21-19l-8-6q-2-2-15 14l-13 14z"/></defs><g stroke="currentcolor" fill="currentcolor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-9-TEX-I-78"/></g><g data-mml-node="mo" transform="translate(63.8, -29)"><use xlink:href="#MJX-9-TEX-N-5E"/></g></g></g></g></g></svg><b>xhat</b></a><nav><ul><li><a href=/ class=active>文章</a></li><li><a href=/tags/>标签</a></li><li><a href=/archives/>归档</a></li><li><a href=/about/>关于</a></li></ul></nav></header><section class=main><article class=content><h2 class=title>理解 RNN</h2><div class=remark><time datetime="2022-04-21 11:38:04 +0800 +0800">2022-04-21</time></div><div class=tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div><div class=markdown-body><p>循环神经网络 (Recurrent Neural Network, RNN) 是一种用于处理时间序列数据的神经网络结构。包括文字、语音、视频等对象。这些数据有两个主要特点：</p><ul><li>数据无固定长度</li><li>数据是有时序的，相邻数据之间存在相关性，非相互独立</li></ul><p>考虑这样一个问题，如果要预测句子下一个单词是什么，一般需要用到当前单词以及前面的单词，因为句子中前后单词并不是独立的。比如，当前单词是“很”，前一个单词是“天空”，那么下一个单词很大概率是“蓝”。循环神经网络就像人一样拥有记忆的能力，它的输出依赖于当前的输入和记忆，刻画出一个序列当前的输出与之前信息的关系。</p><p>循环神经网络适用于许多序列问题中，例如文本处理、语音识别以及机器翻译等。</p><h2 id=基本结构>基本结构</h2><p align=center><img src=/images/rnn_scratch.jpg></p><p>如果把上面有 W 的那个带箭头的圈去掉，它就变成了普通的全连接神经网络。图中每个圆圈可以看作是一个单元，而且每个单元做的事情也是一样的，因此可以折叠呈左半图的样子。用一句话解释 RNN，就是一个单元结构重复使用。</p><p>简单理清一下各符号的定义：</p><ul><li>$x_t$ 表示 t 时刻的输入</li><li>$y_t$ 表示 t 时刻的输出</li><li>$s_t$ 表示 t 时刻的记忆，即隐藏层的输出</li><li>U 是输入层到隐藏层之间的权重矩阵</li><li>W 是记忆单元到隐藏层之间的权重矩阵</li><li>V 是隐藏层到输出层之间的权重矩阵</li><li>U, W, V 都是权重矩阵，在不同时刻 t 之间是<strong>共享权重</strong>的</li></ul><p>从右半图可以看到，RNN 每个时刻隐藏层输出都会传递给下一时刻，因此每个时刻的网络都会保留一定的来自之前时刻的历史信息，并结合当前时刻的网络状态一并再传给下一时刻。</p><p>比如在文本预测中，文本序列为 &ldquo;machine&rdquo;，则输入序列和标签序列分别为 &ldquo;machin&rdquo; 和 &ldquo;achine&rdquo;，网络的概览图如下：<p align=center><img src=/images/rnn-train.svg></p></p><h2 id=前向计算>前向计算</h2><p align=center><img src=/images/rnn-step-forward.png height=300></p><p>在一个循环神经网络中，假设隐藏层只有一层。在 t 时刻神经网络接收到一个输入 $x_t$，则隐藏层的输出 $s_t$ 为：
$$
s_t = \tanh(Ux_t + Ws_{t-1} + b_s)
$$</p><p>在神经网络刚开始训练时，记忆单元中没有上一时刻的网络状态，这时候 $s_{t-1}$ 就是一个初始值。</p><p>在得到隐藏层的输出后，神经网络的输出 $y_t$ 为：
$$
y_t = \mathrm{softmax}(Vs_t + b_y)
$$</p><p>设 t 时刻的输入 $x_t$ 的维度为 $(n_x, m)$，其中 $n_x$ 为变量 $x$ 的维度（输入层节点数量），$m$ 为样本数量；隐藏层节点的数量为 $n_s$；输出层节点数量为 $n_y$。则各变量的信息如下：</p><table><thead><tr><th>变量</th><th>说明</th><th>维度</th></tr></thead><tbody><tr><td>$x_t$</td><td>t 时刻输入</td><td>$(n_x, m)$</td></tr><tr><td>$s_t$</td><td>t 时刻隐藏层输出</td><td>$(n_s, m)$</td></tr><tr><td>$U$</td><td>输入层到隐藏层之间的权重矩阵</td><td>$(n_s, n_x)$</td></tr><tr><td>$W$</td><td>记忆单元到隐藏层之间的权重矩阵</td><td>$(n_s, n_s)$</td></tr><tr><td>$V$</td><td>隐藏层到输出层之间的权重矩阵</td><td>$(n_y, n_s)$</td></tr><tr><td>$b_s$</td><td>隐藏层偏置量</td><td>$(n_s, 1)$</td></tr><tr><td>$b_y$</td><td>输出层偏置量</td><td>$(n_y, 1)$</td></tr></tbody></table><p>某个时刻的 RNN 前向计算流程如下：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>rnn_cell_forward</span>(xt, s_prev, parameters):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d14>    Implements a single forward step of the RNN-cell
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#d14>    xt -- Your input data at timestep &#34;t&#34;, numpy array of shape (n_x, m).
</span></span></span><span style=display:flex><span><span style=color:#d14>    s_prev -- Hidden state at timestep &#34;t-1&#34;, numpy array of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    parameters -- Python dictionary containing:
</span></span></span><span style=display:flex><span><span style=color:#d14>        U -- Weight matrix multiplying the input, numpy array of shape (n_s, n_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>        W -- Weight matrix multiplying the hidden state, numpy array of shape (n_s, n_s)
</span></span></span><span style=display:flex><span><span style=color:#d14>        V -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_s)
</span></span></span><span style=display:flex><span><span style=color:#d14>        bs -- Bias, numpy array of shape (n_s, 1)
</span></span></span><span style=display:flex><span><span style=color:#d14>        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
</span></span></span><span style=display:flex><span><span style=color:#d14>    
</span></span></span><span style=display:flex><span><span style=color:#d14>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#d14>    s_next -- Next hidden state, of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    yt_pred -- Prediction at timestep &#34;t&#34;, numpy array of shape (n_y, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    cache -- Tuple of values needed for the backward pass, contains (s_next, s_prev, xt, parameters)
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    U <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;U&#34;</span>]
</span></span><span style=display:flex><span>    W <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;W&#34;</span>]
</span></span><span style=display:flex><span>    V <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;V&#34;</span>]
</span></span><span style=display:flex><span>    bs <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;bs&#34;</span>]
</span></span><span style=display:flex><span>    by <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;by&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Compute next hidden state</span>
</span></span><span style=display:flex><span>    s_next <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>tanh(np<span style=color:#000;font-weight:700>.</span>dot(W, a_prev) <span style=color:#000;font-weight:700>+</span> np<span style=color:#000;font-weight:700>.</span>dot(U, xt) <span style=color:#000;font-weight:700>+</span> bs)
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Compute output of the current cell</span>
</span></span><span style=display:flex><span>    yt_pred <span style=color:#000;font-weight:700>=</span> softmax(np<span style=color:#000;font-weight:700>.</span>dot(V, s_next) <span style=color:#000;font-weight:700>+</span> by)
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Store values needed for backward propagation in cache</span>
</span></span><span style=display:flex><span>    cache <span style=color:#000;font-weight:700>=</span> (s_next, s_prev, xt, parameters)
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> s_next, yt_pred, cache
</span></span></code></pre></div><p>接下来考虑沿时间线前向计算的过程，设总的时间步长为 $T_x$，此时网络的输入 $x$ 的维度为 $(n_x, m, T_x)$，$s_0$ 为初始隐藏状态，可以使用一些策略进行初始化：</p><ul><li>置 0</li><li>随机值</li><li>作为模型参数训练学习</li></ul><p>通常采用零向量或随机向量作为初始状态，但通过学习得出的初始值可能效果更好，见 <a href=https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html>6</a>。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>rnn_forward</span>(x, s0, parameters):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d14>    Implement the forward propagation of the recurrent neural network
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#d14>    x -- Input data for every time-step, of shape (n_x, m, T_x).
</span></span></span><span style=display:flex><span><span style=color:#d14>    s0 -- Initial hidden state, of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    parameters -- See `rnn_cell_forward`
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#d14>    s -- Hidden states for every time-step, numpy array of shape (n_s, m, T_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>    caches -- Tuple of values needed for the backward pass, contains (list of caches, x)
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Initialize &#34;caches&#34; which will contain the list of all caches</span>
</span></span><span style=display:flex><span>    caches <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    n_x, m, T_x <span style=color:#000;font-weight:700>=</span> x<span style=color:#000;font-weight:700>.</span>shape
</span></span><span style=display:flex><span>    n_y, n_s <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;V&#34;</span>]<span style=color:#000;font-weight:700>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    s <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_s, m, T_x))
</span></span><span style=display:flex><span>    y_pred <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_y, m, T_x))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    s_next <span style=color:#000;font-weight:700>=</span> s0
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># loop over all time-steps</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>for</span> t <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(T_x):
</span></span><span style=display:flex><span>        <span style=color:#998;font-style:italic># Update next hidden state, compute the prediction</span>
</span></span><span style=display:flex><span>        s_next, yt_pred, cache <span style=color:#000;font-weight:700>=</span> rnn_cell_forward(x[:,:,t], s_next, parameters)
</span></span><span style=display:flex><span>        s[:,:,t] <span style=color:#000;font-weight:700>=</span> s_next
</span></span><span style=display:flex><span>        y_pred[:,:,t] <span style=color:#000;font-weight:700>=</span> yt_pred
</span></span><span style=display:flex><span>        caches<span style=color:#000;font-weight:700>.</span>append(cache)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    caches <span style=color:#000;font-weight:700>=</span> (caches, x)
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> s, y_pred, caches
</span></span></code></pre></div><h2 id=反向传播>反向传播</h2><h2 id=示例>示例</h2><h2 id=结论>结论</h2><h2 id=参考>参考</h2><p>[1] <a href=https://www.bilibili.com/read/cv15812073>开小灶｜循环神经网络RNN讲解(一)</a></p><p>[2] <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjMwODMyMQ==&mid=2456338385&idx=1&sn=8e9194c87d3ac6f9134c112b28724e0c&chksm=8c2fc5dfbb584cc9dddbcaea7da777157437f59637f189f2d40f76606c4f52be4b886973bf91&scene=21#wechat_redirect">RNN神经网络模型综述</a></p><p>[3] <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/>The Unreasonable Effectiveness of Recurrent Neural Networks</a></p><p>[4] <a href=https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html>循环神经网络 - 动手学深度学习</a></p><p>[5] <a href=https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html>Building your Recurrent Neural Network - Step by Step</a></p><p>[6] <a href=https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html>Non-Zero Initial States for Recurrent Neural Networks</a></p><p>[7] <a href=https://danijar.com/tips-for-training-recurrent-neural-networks/>Tips for Training Recurrent Neural Networks</a></p></div><link href=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.12.0/katex.min.css rel=stylesheet><script src=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.12.0/katex.min.js></script>
<script src=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js onload='renderMathInElement(document.querySelector(`.content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script></article></section><section class=tail><div class=content id=post-nav><a title="使用 MQTT 实现 API 接口" rel=prev href=/posts/mqtt-create-api/><span>前篇</span>使用 MQTT 实现 API 接口</a></div><div class=content id=post-comment><script src=https://giscus.app/client.js data-repo=koho/blog data-repo-id="MDEwOlJlcG9zaXRvcnkzMjM1MDUxMDk=" data-category=Announcements data-category-id=DIC_kwDOE0hL1c4COeTz data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class=content id=post-related><h3 class=title>延伸阅读</h3><div class=grid><div class=card><a href=/posts/introduce-to-weighted-least-squares/ rel=bookmark><div class=banner style=background-image:url(/images/0_g0BN7JUBfBoDkriE.png)></div><h4>加权最小二乘</h4><p>2020-11-30</p><div class=tags><span>机器学习</span>
<span>回归</span></div></a></div></div></div></section><footer><span>Powered by <a href=https://gohugo.io>Hugo</a></span><br><span>© 2022 xhat</span></footer></body></html>