<!doctype html><html lang=zh-CN id=page><head><meta charset=utf-8><title>理解 RNN - X̂ Blog</title><meta name=viewport content="width=device-width,initial-scale=1"><link href=/css/font.css type=text/css rel=stylesheet><link href=/css/main.css type=text/css rel=stylesheet><link href=/css/github-markdown-light.css type=text/css rel=stylesheet><link rel=icon href=/favicon.svg></head><body><header><a href=/ id=logo><svg xmlns="http://www.w3.org/2000/svg" width="38.82" height="52.68" viewBox="0 -765 572 776" xmlns:xlink="http://www.w3.org/1999/xlink" style><defs><path id="MJX-9-TEX-I-78" d="M52 289q7 42 54 97t116 56q35 0 64-18t43-45q42 63 101 63 37 0 64-22t28-59q0-29-14-47t-27-22-23-4q-19 0-31 11t-12 29q0 46 50 63-11 13-40 13-13 0-19-2-38-16-56-66-60-221-60-258 0-28 16-40t35-12q37 0 73 33t49 81q3 10 6 11t16 2h4q15 0 15-8 0-1-2-11-16-57-62-101T333-11q-70 0-106 63-41-62-94-62h-6Q78-10 57 16T35 71q0 32 19 52t45 20q43 0 43-42 0-20-12-35T107 46 94 41l-3-1q0-1 6-4t16-7 19-3q36 0 62 45 9 16 23 68t28 108 16 66q5 27 5 39 0 28-15 40t-34 12q-40 0-75-32T93 290q-2-9-5-10t-16-2H58q-6 6-6 11z"/><path id="MJX-9-TEX-N-5E" d="M112 560 249 694l8-8Q387 562 387 560l-26-29q-2 1-58 50l-53 46-55-47q-13-11-26-23t-21-19l-8-6q-2-2-15 14l-13 14z"/></defs><g stroke="currentcolor" fill="currentcolor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom"><g data-mml-node="mover"><g data-mml-node="mi"><use xlink:href="#MJX-9-TEX-I-78"/></g><g data-mml-node="mo" transform="translate(63.8, -29)"><use xlink:href="#MJX-9-TEX-N-5E"/></g></g></g></g></g></svg><b>xhat</b></a><nav><ul><li><a href=/ class=active>文章</a></li><li><a href=/tags/>标签</a></li><li><a href=/archives/>归档</a></li><li><a href=/about/>关于</a></li></ul></nav></header><section class=main><article class=content><h2 class=title>理解 RNN</h2><div class=remark><time datetime="2022-04-21 11:38:04 +0800 +0800">2022-04-21</time></div><div class=tags><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div><div class=markdown-body><p>循环神经网络 (Recurrent Neural Network, RNN) 是一种用于处理时间序列数据的神经网络结构。包括文字、语音、视频等对象。这些数据有两个主要特点：</p><ul><li>数据无固定长度</li><li>数据是有时序的，相邻数据之间存在相关性，非相互独立</li></ul><p>考虑这样一个问题，如果要预测句子下一个单词是什么，一般需要用到当前单词以及前面的单词，因为句子中前后单词并不是独立的。比如，当前单词是“很”，前一个单词是“天空”，那么下一个单词很大概率是“蓝”。循环神经网络就像人一样拥有记忆的能力，它的输出依赖于当前的输入和记忆，刻画出一个序列当前的输出与之前信息的关系。</p><p>循环神经网络适用于许多序列问题中，例如文本处理、语音识别以及机器翻译等。</p><h2 id=基本结构>基本结构</h2><p align=center><img src=/images/rnn_scratch.jpg></p><p>如果把上面有 W 的那个带箭头的圈去掉，它就变成了普通的全连接神经网络。图中每个圆圈可以看作是一个单元，而且每个单元做的事情也是一样的，因此可以折叠呈左半图的样子。用一句话解释 RNN，就是一个单元结构重复使用。</p><p>简单理清一下各符号的定义：</p><ul><li>$x_t$ 表示 t 时刻的输入</li><li>$y_t$ 表示 t 时刻的输出</li><li>$s_t$ 表示 t 时刻的记忆，即隐藏层的输出</li><li>U 是输入层到隐藏层之间的权重矩阵</li><li>W 是记忆单元到隐藏层之间的权重矩阵</li><li>V 是隐藏层到输出层之间的权重矩阵</li><li>U, W, V 都是权重矩阵，在不同时刻 t 之间是<strong>共享权重</strong>的</li></ul><p>从右半图可以看到，RNN 每个时刻隐藏层输出都会传递给下一时刻，因此每个时刻的网络都会保留一定的来自之前时刻的历史信息，并结合当前时刻的网络状态一并再传给下一时刻。</p><p>比如在文本预测中，文本序列为 &ldquo;machine&rdquo;，则输入序列和标签序列分别为 &ldquo;machin&rdquo; 和 &ldquo;achine&rdquo;，网络的概览图如下：<p align=center><img src=/images/rnn-train.svg></p></p><h2 id=前向计算>前向计算</h2><p align=center><img src=/images/rnn-step-forward.png height=300></p><p>在一个循环神经网络中，假设隐藏层只有一层。在 t 时刻神经网络接收到一个输入 $x_t$，则隐藏层的输出 $s_t$ 为：
$$
s_t = \tanh(Ux_t + Ws_{t-1} + b_s)
$$</p><p>在神经网络刚开始训练时，记忆单元中没有上一时刻的网络状态，这时候 $s_{t-1}$ 就是一个初始值。</p><p>在得到隐藏层的输出后，神经网络的输出 $y_t$ 为：
$$
y_t = \mathrm{softmax}(Vs_t + b_y)
$$</p><p>设 t 时刻的输入 $x_t$ 的维度为 $(n_x, m)$，其中 $n_x$ 为变量 $x$ 的维度（输入层节点数量），$m$ 为样本数量；隐藏层节点的数量为 $n_s$；输出层节点数量为 $n_y$。则各变量的信息如下：</p><table><thead><tr><th>变量</th><th>说明</th><th>维度</th></tr></thead><tbody><tr><td>$x_t$</td><td>t 时刻输入</td><td>$(n_x, m)$</td></tr><tr><td>$s_t$</td><td>t 时刻隐藏层输出</td><td>$(n_s, m)$</td></tr><tr><td>$U$</td><td>输入层到隐藏层之间的权重矩阵</td><td>$(n_s, n_x)$</td></tr><tr><td>$W$</td><td>记忆单元到隐藏层之间的权重矩阵</td><td>$(n_s, n_s)$</td></tr><tr><td>$V$</td><td>隐藏层到输出层之间的权重矩阵</td><td>$(n_y, n_s)$</td></tr><tr><td>$b_s$</td><td>隐藏层偏置量</td><td>$(n_s, 1)$</td></tr><tr><td>$b_y$</td><td>输出层偏置量</td><td>$(n_y, 1)$</td></tr></tbody></table><p>某个时刻的 RNN 前向计算流程如下：</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>rnn_cell_forward</span>(xt, s_prev, parameters):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d14>    Implements a single forward step of the RNN-cell
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#d14>    xt -- Your input data at timestep &#34;t&#34;, numpy array of shape (n_x, m).
</span></span></span><span style=display:flex><span><span style=color:#d14>    s_prev -- Hidden state at timestep &#34;t-1&#34;, numpy array of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    parameters -- Python dictionary containing:
</span></span></span><span style=display:flex><span><span style=color:#d14>        U -- Weight matrix multiplying the input, numpy array of shape (n_s, n_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>        W -- Weight matrix multiplying the hidden state, numpy array of shape (n_s, n_s)
</span></span></span><span style=display:flex><span><span style=color:#d14>        V -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_s)
</span></span></span><span style=display:flex><span><span style=color:#d14>        bs -- Bias, numpy array of shape (n_s, 1)
</span></span></span><span style=display:flex><span><span style=color:#d14>        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
</span></span></span><span style=display:flex><span><span style=color:#d14>    
</span></span></span><span style=display:flex><span><span style=color:#d14>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#d14>    s_next -- Next hidden state, of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    yt_pred -- Prediction at timestep &#34;t&#34;, numpy array of shape (n_y, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    cache -- Tuple of values needed for the backward pass, contains (s_next, s_prev, xt, parameters)
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    U <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;U&#34;</span>]
</span></span><span style=display:flex><span>    W <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;W&#34;</span>]
</span></span><span style=display:flex><span>    V <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;V&#34;</span>]
</span></span><span style=display:flex><span>    bs <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;bs&#34;</span>]
</span></span><span style=display:flex><span>    by <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;by&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Compute next hidden state</span>
</span></span><span style=display:flex><span>    s_next <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>tanh(np<span style=color:#000;font-weight:700>.</span>dot(W, s_prev) <span style=color:#000;font-weight:700>+</span> np<span style=color:#000;font-weight:700>.</span>dot(U, xt) <span style=color:#000;font-weight:700>+</span> bs)
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Compute output of the current cell</span>
</span></span><span style=display:flex><span>    yt_pred <span style=color:#000;font-weight:700>=</span> softmax(np<span style=color:#000;font-weight:700>.</span>dot(V, s_next) <span style=color:#000;font-weight:700>+</span> by)
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Store values needed for backward propagation in cache</span>
</span></span><span style=display:flex><span>    cache <span style=color:#000;font-weight:700>=</span> (s_next, s_prev, xt, parameters)
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> s_next, yt_pred, cache
</span></span></code></pre></div><p>接下来考虑沿时间线前向计算的过程，设总的时间步长为 $T_x$，此时网络的输入 $x$ 的维度为 $(n_x, m, T_x)$，$s_0$ 为初始隐藏状态，可以使用一些策略进行初始化：</p><ul><li>置 0</li><li>随机值</li><li>作为模型参数训练学习</li></ul><p>通常采用零向量或随机向量作为初始状态，但通过学习得出的初始值可能效果更好，见 <a href=https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html>6</a>。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>rnn_forward</span>(x, s0, parameters):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d14>    Implement the forward propagation of the recurrent neural network
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#d14>    x -- Input data for every time-step, of shape (n_x, m, T_x).
</span></span></span><span style=display:flex><span><span style=color:#d14>    s0 -- Initial hidden state, of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>    parameters -- See `rnn_cell_forward`
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#d14>    s -- Hidden states for every time-step, numpy array of shape (n_s, m, T_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>    caches -- Tuple of values needed for the backward pass, contains (list of caches, x)
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Initialize &#34;caches&#34; which will contain the list of all caches</span>
</span></span><span style=display:flex><span>    caches <span style=color:#000;font-weight:700>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    n_x, m, T_x <span style=color:#000;font-weight:700>=</span> x<span style=color:#000;font-weight:700>.</span>shape
</span></span><span style=display:flex><span>    n_y, n_s <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;V&#34;</span>]<span style=color:#000;font-weight:700>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    s <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_s, m, T_x))
</span></span><span style=display:flex><span>    y_pred <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_y, m, T_x))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    s_next <span style=color:#000;font-weight:700>=</span> s0
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># loop over all time-steps</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>for</span> t <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>range</span>(T_x):
</span></span><span style=display:flex><span>        <span style=color:#998;font-style:italic># Update next hidden state, compute the prediction</span>
</span></span><span style=display:flex><span>        s_next, yt_pred, cache <span style=color:#000;font-weight:700>=</span> rnn_cell_forward(x[:,:,t], s_next, parameters)
</span></span><span style=display:flex><span>        s[:,:,t] <span style=color:#000;font-weight:700>=</span> s_next
</span></span><span style=display:flex><span>        y_pred[:,:,t] <span style=color:#000;font-weight:700>=</span> yt_pred
</span></span><span style=display:flex><span>        caches<span style=color:#000;font-weight:700>.</span>append(cache)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    caches <span style=color:#000;font-weight:700>=</span> (caches, x)
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> s, y_pred, caches
</span></span></code></pre></div><h2 id=反向传播>反向传播</h2><p>关于梯度的计算，我们先计算隐藏层输出 $s_t$ 的梯度，然后根据链式法则求得其他梯度。对于某时刻的隐藏层输出 $s_t$，误差的来源有两个：一个是当前时刻的输出层传递过来的误差，另一个是后面时刻的输出层传递过来的误差。总的来说，就是把大于等于 t 时刻的所有误差都计算进去。</p><p>例如，考虑 t - 3 时刻的梯度计算：</p>$$
\begin{aligned}
\frac{\partial E}{\partial s_{t-3}} &= \frac{\partial L}{\partial s_{t}}
\frac{\partial s_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial s_{t-2}}
\frac{\partial s_{t-2}}{\partial s_{t-3}} + \frac{\partial L}{\partial s_{t-1}}
\frac{\partial s_{t-1}}{\partial s_{t-2}}
\frac{\partial s_{t-2}}{\partial s_{t-3}} +
\frac{\partial L}{\partial s_{t-2}}
\frac{\partial s_{t-2}}{\partial s_{t-3}} +
\frac{\partial L}{\partial s_{t-3}}\\
&=\frac{\partial s_{t-2}}{\partial s_{t-3}} \left(\frac{\partial L}{\partial s_{t}}
\frac{\partial s_t}{\partial s_{t-1}}\frac{\partial s_{t-1}}{\partial s_{t-2}} +
\frac{\partial L}{\partial s_{t-1}}
\frac{\partial s_{t-1}}{\partial s_{t-2}} +
\frac{\partial L}{\partial s_{t-2}}
\right) + \frac{\partial E}{\partial s_{t-3}}\\
&=\frac{\partial E}{\partial s_{t-2}}\frac{\partial s_{t-2}}{\partial s_{t-3}} + \frac{\partial L}{\partial s_{t-3}}
\end{aligned}
$$<p>可以看出这是梯度的递归式，最后梯度的两项和上面的描述一致。我们只要从最后的 t 时刻一步一步往前面的时刻推算，就能得到所有时刻的梯度。上式还有一项需要计算，由于
$$
\frac{\partial \tanh(x)}{\partial x} = 1 - \tanh(x)^2
$$</p><p>参照前向计算中 $s_t$ 的表达式，则 $s_t$ 关于上一时刻 $s_{t-1}$ 的导数容易求得：
$$
\frac{\partial s_{t}}{\partial s_{t-1}} = W^T(1 - s_t^2)
$$</p><p>最后可得各权重的梯度：
$$
\frac{\partial s_{t}}{\partial W} = (1 - s_t^2)s_{t-1}^T
$$
$$
\frac{\partial s_{t}}{\partial U} = (1 - s_t^2)x_{t}^T
$$
$$
\frac{\partial s_{t}}{\partial b_s} = \sum_{batch} (1 - s_t^2)
$$</p><p>具体实现方面，可以分为两部分，一部分是单个 RNN 单元里面的反向传播；另一部分是沿时间线的反向传播。</p><h3 id=单元内传播>单元内传播</h3><p>设当前单元的时刻为 t，我们假设此时已经计算出了当前隐藏层输出 $s_t$ 的梯度 $\frac{\partial E}{\partial s_t}$，各权重的梯度可以立刻得出：
$$
\frac{\partial E}{\partial W} = \frac{\partial E}{\partial s_t} \frac{\partial s_{t}}{\partial W} = \frac{\partial E}{\partial s_t} (1 - s_t^2)s_{t-1}^T
$$
$$
\frac{\partial E}{\partial U} = \frac{\partial E}{\partial s_t} \frac{\partial s_{t}}{\partial U} = \frac{\partial E}{\partial s_t} (1 - s_t^2)x_{t}^T
$$
$$
\frac{\partial E}{\partial b_s} = \frac{\partial E}{\partial s_t} \frac{\partial s_{t}}{\partial b_s} = \frac{\partial E}{\partial s_t} \sum_{batch} (1 - s_t^2)
$$</p><p>值得注意的是还要计算关于上一个时刻 t - 1 隐藏层输出的梯度（式 3）的第一项：
$$
\frac{\partial E}{\partial s_t} \frac{\partial s_{t}}{\partial s_{t-1}} = W_T \frac{\partial E}{\partial s_t} (1 - s_t^2)
$$</p><p>上面的式子都有公共项，可以提取出来：
$$
\text{dtanh} = \frac{\partial E}{\partial s_t} (1 - s_t^2)
$$</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>rnn_cell_backward</span>(ds_next, cache):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d14>    Implements the backward pass for the RNN-cell (single time-step).
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#d14>    ds_next -- Gradient of loss with respect to next hidden state
</span></span></span><span style=display:flex><span><span style=color:#d14>    cache -- python dictionary containing useful values (output of rnn_cell_forward())
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#d14>    gradients -- python dictionary containing:
</span></span></span><span style=display:flex><span><span style=color:#d14>        dx -- Gradients of input data, of shape (n_x, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>        ds_prev -- Gradients of previous hidden state, of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>        dU -- Gradients of input-to-hidden weights, of shape (n_s, n_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>        dW -- Gradients of hidden-to-hidden weights, of shape (n_s, n_s)
</span></span></span><span style=display:flex><span><span style=color:#d14>        dbs -- Gradients of bias vector, of shape (n_s, 1)
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Retrieve values from cache</span>
</span></span><span style=display:flex><span>    (s_next, s_prev, xt, parameters) <span style=color:#000;font-weight:700>=</span> cache
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Retrieve values from parameters</span>
</span></span><span style=display:flex><span>    U <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;U&#34;</span>]
</span></span><span style=display:flex><span>    W <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;W&#34;</span>]
</span></span><span style=display:flex><span>    V <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;V&#34;</span>]
</span></span><span style=display:flex><span>    bs <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;bs&#34;</span>]
</span></span><span style=display:flex><span>    by <span style=color:#000;font-weight:700>=</span> parameters[<span style=color:#d14>&#34;by&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dtanh <span style=color:#000;font-weight:700>=</span> (<span style=color:#099>1</span> <span style=color:#000;font-weight:700>-</span> s_next <span style=color:#000;font-weight:700>**</span> <span style=color:#099>2</span>) <span style=color:#000;font-weight:700>*</span> ds_next
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dxt <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>dot(U<span style=color:#000;font-weight:700>.</span>T, dtanh)
</span></span><span style=display:flex><span>    dU <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>dot(dtanh, xt<span style=color:#000;font-weight:700>.</span>T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    ds_prev <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>dot(W<span style=color:#000;font-weight:700>.</span>T, dtanh)
</span></span><span style=display:flex><span>    dW <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>dot(dtanh, s_prev<span style=color:#000;font-weight:700>.</span>T)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    dbs <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>sum(dtanh, <span style=color:#099>1</span>, keepdims<span style=color:#000;font-weight:700>=</span><span style=color:#000;font-weight:700>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    gradients <span style=color:#000;font-weight:700>=</span> {<span style=color:#d14>&#34;dxt&#34;</span>: dxt, <span style=color:#d14>&#34;ds_prev&#34;</span>: ds_prev, <span style=color:#d14>&#34;dU&#34;</span>: dU, <span style=color:#d14>&#34;dW&#34;</span>: dW, <span style=color:#d14>&#34;dbs&#34;</span>: dbs}
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> gradients
</span></span></code></pre></div><h3 id=沿时间线传播>沿时间线传播</h3><p>沿时间线传播的流程是把误差从最后的 t 时刻一步一步往前传播到 0 时刻。假设此时已经计算出了各时刻输出层损失函数对隐藏层输出 $s_t$ 的梯度 $\frac{\partial L}{\partial s_t}$（式 3）的第二项。</p><p>则我们在循环每个时刻时，加上上一个单元对当前时刻隐藏层梯度（式 3 的第一项），可计算出式 3 的值，从而进行上面的单元内传播。</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>rnn_backward</span>(ds, caches):
</span></span><span style=display:flex><span>    <span style=color:#d14>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#d14>    Implement the backward pass for a RNN over an entire sequence of input data.
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Arguments:
</span></span></span><span style=display:flex><span><span style=color:#d14>    ds -- Upstream gradients of all hidden states, of shape (n_s, m, T_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>    caches -- tuple containing information from the forward pass (rnn_forward)
</span></span></span><span style=display:flex><span><span style=color:#d14>
</span></span></span><span style=display:flex><span><span style=color:#d14>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#d14>    gradients -- python dictionary containing:
</span></span></span><span style=display:flex><span><span style=color:#d14>        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>        ds0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_s, m)
</span></span></span><span style=display:flex><span><span style=color:#d14>        dU -- Gradient w.r.t the input&#39;s weight matrix, numpy-array of shape (n_s, n_x)
</span></span></span><span style=display:flex><span><span style=color:#d14>        dW -- Gradient w.r.t the hidden state&#39;s weight matrix, numpy-arrayof shape (n_s, n_s)
</span></span></span><span style=display:flex><span><span style=color:#d14>        dbs -- Gradient w.r.t the bias, of shape (n_s, 1)
</span></span></span><span style=display:flex><span><span style=color:#d14>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    (caches, x) <span style=color:#000;font-weight:700>=</span> caches
</span></span><span style=display:flex><span>    (s1, s0, x1, parameters) <span style=color:#000;font-weight:700>=</span> caches[<span style=color:#099>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    n_s, m, T_x <span style=color:#000;font-weight:700>=</span> ds<span style=color:#000;font-weight:700>.</span>shape
</span></span><span style=display:flex><span>    n_x, m <span style=color:#000;font-weight:700>=</span> x1<span style=color:#000;font-weight:700>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Initialize the gradients with the right sizes</span>
</span></span><span style=display:flex><span>    dx <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_x, m, T_x))
</span></span><span style=display:flex><span>    dU <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_s, n_x))
</span></span><span style=display:flex><span>    dW <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_s, n_s))
</span></span><span style=display:flex><span>    dbs <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_s, <span style=color:#099>1</span>))
</span></span><span style=display:flex><span>    ds0 <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_s, m))
</span></span><span style=display:flex><span>    ds_prevt <span style=color:#000;font-weight:700>=</span> np<span style=color:#000;font-weight:700>.</span>zeros((n_s, m))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Loop through all the time steps</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>for</span> t <span style=color:#000;font-weight:700>in</span> <span style=color:#0086b3>reversed</span>(<span style=color:#0086b3>range</span>(T_x)):
</span></span><span style=display:flex><span>        <span style=color:#998;font-style:italic># Compute gradients at time step t.</span>
</span></span><span style=display:flex><span>        gradients <span style=color:#000;font-weight:700>=</span> rnn_cell_backward(ds[:, :, t] <span style=color:#000;font-weight:700>+</span> ds_prevt, caches[t])
</span></span><span style=display:flex><span>        <span style=color:#998;font-style:italic># Retrieve derivatives from gradients</span>
</span></span><span style=display:flex><span>        dxt, ds_prevt, dUt, dWt, dbst <span style=color:#000;font-weight:700>=</span> gradients[<span style=color:#d14>&#34;dxt&#34;</span>], gradients[<span style=color:#d14>&#34;ds_prev&#34;</span>], gradients[<span style=color:#d14>&#34;dU&#34;</span>], gradients[<span style=color:#d14>&#34;dW&#34;</span>], gradients[<span style=color:#d14>&#34;dbs&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#998;font-style:italic># Increment global derivatives w.r.t parameters by adding their derivative at time-step t</span>
</span></span><span style=display:flex><span>        dx[:, :, t] <span style=color:#000;font-weight:700>=</span> dxt
</span></span><span style=display:flex><span>        dU <span style=color:#000;font-weight:700>+=</span> dUt
</span></span><span style=display:flex><span>        dW <span style=color:#000;font-weight:700>+=</span> dWt
</span></span><span style=display:flex><span>        dbs <span style=color:#000;font-weight:700>+=</span> dbst
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Set da0 to the gradient of a which has been backpropagated through all time-steps</span>
</span></span><span style=display:flex><span>    ds0 <span style=color:#000;font-weight:700>=</span> ds_prevt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#998;font-style:italic># Store the gradients in a python dictionary</span>
</span></span><span style=display:flex><span>    gradients <span style=color:#000;font-weight:700>=</span> {<span style=color:#d14>&#34;dx&#34;</span>: dx, <span style=color:#d14>&#34;ds0&#34;</span>: ds0, <span style=color:#d14>&#34;dU&#34;</span>: dU, <span style=color:#d14>&#34;dW&#34;</span>: dW, <span style=color:#d14>&#34;dbs&#34;</span>: dbs}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>return</span> gradients
</span></span></code></pre></div><h2 id=结论>结论</h2><h3 id=优点>优点</h3><ul><li>计算考虑了历史信息</li><li>适合处理序列数据</li><li>可处理任意长度的输入</li></ul><h3 id=缺点>缺点</h3><ul><li>梯度消失、梯度爆炸</li><li>无法处理长时依赖问题</li><li>计算速度慢</li></ul><h2 id=参考>参考</h2><p>[1] <a href=https://www.bilibili.com/read/cv15812073>开小灶｜循环神经网络RNN讲解(一)</a></p><p>[2] <a href="https://mp.weixin.qq.com/s?__biz=MzAxMjMwODMyMQ==&amp;mid=2456338385&amp;idx=1&amp;sn=8e9194c87d3ac6f9134c112b28724e0c&amp;chksm=8c2fc5dfbb584cc9dddbcaea7da777157437f59637f189f2d40f76606c4f52be4b886973bf91&amp;scene=21#wechat_redirect">RNN神经网络模型综述</a></p><p>[3] <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/>The Unreasonable Effectiveness of Recurrent Neural Networks</a></p><p>[4] <a href=https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html>循环神经网络 - 动手学深度学习</a></p><p>[5] <a href=https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html>Building your Recurrent Neural Network - Step by Step</a></p><p>[6] <a href=https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html>Non-Zero Initial States for Recurrent Neural Networks</a></p><p>[7] <a href=https://danijar.com/tips-for-training-recurrent-neural-networks/>Tips for Training Recurrent Neural Networks</a></p></div><link href=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.12.0/katex.min.css rel=stylesheet><script src=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.12.0/katex.min.js></script>
<script src=https://cdn.bootcdn.net/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js onload='renderMathInElement(document.querySelector(`.content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script></article></section><section class=tail><div class=content id=post-nav><a title="使用 MQTT 实现 API 接口" rel=prev href=/posts/mqtt-create-api/><span>前篇</span>使用 MQTT 实现 API 接口</a>
<a title="ECC 椭圆曲线" rel=next href=/posts/elliptic-curve/><span>后篇</span>ECC 椭圆曲线</a></div><div class=content id=post-comment><script src=https://giscus.app/client.js data-repo=koho/blog data-repo-id="MDEwOlJlcG9zaXRvcnkzMjM1MDUxMDk=" data-category=Announcements data-category-id=DIC_kwDOE0hL1c4COeTz data-mapping=pathname data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=light data-lang=zh-CN crossorigin=anonymous async></script></div><div class=content id=post-related><h3 class=title>延伸阅读</h3><div class=grid><div class=card><a href=/posts/introduce-to-weighted-least-squares/ rel=bookmark><div class=banner style=background-image:url(/images/0_g0BN7JUBfBoDkriE.png)></div><h4>加权最小二乘</h4><p>2020-11-30</p><div class=tags><span>机器学习</span>
<span>回归</span></div></a></div></div></div></section><footer><span>Powered by <a href=https://gohugo.io>Hugo</a></span><br><span>© 2023 xhat</span></footer></body></html>